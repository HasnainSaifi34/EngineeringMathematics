{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the polynomial function with given coefficients\n",
    "def Optimalpolynomial(x):\n",
    "    return 2*x**5 - 3*x**4 + x**3 - 5*x**2 + 4*x - 1\n",
    "\n",
    "\n",
    "# Generate data points\n",
    "xx = jnp.linspace(-6, 6, 100)\n",
    "data_points = list(zip(xx, Optimalpolynomial(xx)))\n",
    "\n",
    "x_values = jnp.array([x for x, y in data_points])\n",
    "y_values = jnp.array([y for x, y in data_points])\n",
    "\n",
    "\n",
    "# Plot the original function\n",
    "plt.scatter(x_values, y_values)\n",
    "plt.title(\"Sampled function that needs to be approximated\")\n",
    "plt.show()\n",
    "\n",
    "# Define the polynomial curve to be fitted\n",
    "def Curve(x, arr):\n",
    "    return arr[0] + arr[1]*x + arr[2]*x**2 + arr[3]*x**3 + arr[4]*x**4 + arr[5]*x**5\n",
    "\n",
    "#Huber Loss Function\n",
    "def Loss(arr, delta=1.0):\n",
    "    predictions = Curve(x_values, arr)\n",
    "    error = predictions - y_values\n",
    "    is_small_error = jnp.abs(error) <= delta\n",
    "    squared_loss = jnp.square(error) / 2\n",
    "    linear_loss = delta * (jnp.abs(error) - delta / 2)\n",
    "    return jnp.mean(jnp.where(is_small_error, squared_loss, linear_loss))\n",
    "\n",
    "# Ideal loss with optimal parameters\n",
    "IdealLoss = Loss(jnp.array([-1, 4, -5, 1, -3, 2]))\n",
    "IdealWeights = [-1, 4, -5, 1, -3, 2]\n",
    "print(f'Ideal Loss: {IdealLoss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = - 5.0\n",
    "weights = jnp.array([IdealWeights[0]+shift , IdealWeights[1]+shift ,IdealWeights[2]+shift , IdealWeights[3]+shift , IdealWeights[4]+shift , IdealWeights[5]+shift ])\n",
    "\n",
    "#weights = jnp.array([np.random.normal() * 2.0 for _ in range(6)])\n",
    "\n",
    "# initializing weights close to ideal weights that we want to predict\n",
    "\n",
    "\n",
    "# Define gradient descent step\n",
    "def gradient_descent(weights, gradient, learning_rate, clip_value):\n",
    "    grad_clipped = jnp.clip(gradient, -clip_value, clip_value)  # Limit gradient magnitude\n",
    "    return weights - learning_rate * grad_clipped\n",
    "\n",
    "# Gradient of the loss function\n",
    "gradient = jax.grad(Loss)\n",
    "\n",
    "# Gradient descent parameters\n",
    "tolerance = 1e-10\n",
    "max_iteration = 1000\n",
    "learning_rate = 1e-2   # 1e-2 is getting us good results\n",
    "clip_value = 1.0\n",
    "\n",
    "grad = gradient(weights)\n",
    "\n",
    "loss_values = [Loss(weights)]\n",
    "\n",
    "iteration = 0\n",
    "#Gradient descent loop\n",
    "while jnp.linalg.norm(grad) >= tolerance and (iteration <= max_iteration):\n",
    "    weights = gradient_descent(weights, grad, learning_rate, clip_value)\n",
    "    grad = gradient(weights)\n",
    "    loss = Loss(weights)\n",
    "    loss_values.append(loss)\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'Optimized weights: {weights}')\n",
    "print(f\"Ideal weights: {IdealWeights}\")\n",
    "print(f'Optimized Loss: {Loss(weights)}')\n",
    "# Plot the fitted curve along with the original data points\n",
    "plt.plot(x_values, Curve(x_values, weights), label=\"Predicted\", color='red')\n",
    "plt.scatter(x_values, y_values, label=\"Actual\", color='blue')\n",
    "plt.xlim(-4,+4)\n",
    "plt.ylim(-9,+9)\n",
    "plt.legend()\n",
    "plt.title(\"Fitted curve vs Actual data points\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.title(\"Loss over iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
