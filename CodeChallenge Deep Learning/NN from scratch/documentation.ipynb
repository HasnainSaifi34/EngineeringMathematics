{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's a concise explanation for your documentation:\n",
    "\n",
    "---\n",
    "\n",
    "### NeuralNetwork Class\n",
    "\n",
    "The `NeuralNetwork` class implements a basic feedforward neural network capable of training and making predictions. It accepts the following parameters during initialization:\n",
    "\n",
    "- **`input`**: Represents the input data or features for the neural network.\n",
    "- **`hiddenlayer`**: Specifies the number of neurons in each hidden layer as a list of integers.\n",
    "- **`activation_hidden`**: Tuple containing the activation function and its derivative for the hidden layers.\n",
    "- **`outputlayer`**: Specifies the number of neurons in the output layer.\n",
    "- **`idealValues`**: Target output values for training purposes.\n",
    "- **`activation_output`**: Tuple containing the activation function and its derivative for the output layer.\n",
    "- **`learning_rate`**: Determines the step size in gradient descent optimization during training.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "# Example instantiation\n",
    "NN = NeuralNetwork(\n",
    "    input=[1, 2, 3],\n",
    "    hiddenlayer=[4],\n",
    "    activation_hidden=(activations.LeakyRelu, activations.LeakyRelu_derivative),\n",
    "    outputlayer=2,\n",
    "    idealValues=[1, 2],\n",
    "    activation_output=(activations.sigmoid, activations.sigmoid_derivative),\n",
    "    learning_rate=0.001\n",
    ")\n",
    "```\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The `NeuralNetwork` class encapsulates the functionality for creating and training a neural network model. It provides methods to initialize the network, perform forward and backward propagation, compute loss, and update model parameters using gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/blt790f1b7ac4e04301/6543ff50fcf447040a6b8dc7/News_Image_(47).png?width=1280&auto=webp&quality=95&format=jpg&disable=upscale\"\n",
    "width=500\n",
    "heigth=500\n",
    "/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Sure! Let's go through the forward pass code, explaining each line and the underlying mathematical concepts.\n",
    "\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D4D22AQH-8ATyCCV_Ww/feedshare-shrink_800/0/1720882385473?e=1724284800&v=beta&t=wc2S7yF2Fo5zIDEjMpC-VdjkvJEg9K4m3mTTHPnknDU\"/>\n",
    "\n",
    "### Forward Pass Explained\n",
    "\n",
    "```python\n",
    "def Forwardpass(self):\n",
    "    for i in range(len(self.W) - 1):\n",
    "        Mat_Vec_Mul = self.W[i] @ np.transpose(self.A[i])  # weight and activation(L-1) multiplication , weight matrix of (L) and (L-1)\n",
    "        \n",
    "        shape = Mat_Vec_Mul.shape  # to adjust the shape from (i x 1 ) of vectors to (i,) to reduce errors in computation\n",
    "        Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[i]  # Z = wx + b --> weighted sum\n",
    "        self.Z.append(Z)\n",
    "        A = self.activation_hidden[0](Z)\n",
    "        self.A.append(A)\n",
    "        \n",
    "    Mat_Vec_Mul = self.W[-1] @ np.transpose(self.A[-1]) \n",
    "    shape = Mat_Vec_Mul.shape  # to adjust the shape from (i x 1 ) --> (i,) to reduce errors in computation\n",
    "    Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[-1]\n",
    "    self.Z.append(Z)\n",
    "    A = self.activation_output[0](Z)\n",
    "    self.A.append(A)\n",
    "    Loss = self.Loss()\n",
    "    return Loss\n",
    "```\n",
    "\n",
    "### Mathematical Explanation and Chain Rule Application\n",
    "\n",
    "#### 1. Initialize the Forward Pass\n",
    "\n",
    "```python\n",
    "for i in range(len(self.W) - 1):\n",
    "```\n",
    "- **Explanation**: Iterate through each layer except the last one (output layer).\n",
    "\n",
    "#### 2. Weighted Sum Calculation\n",
    "\n",
    "```python\n",
    "Mat_Vec_Mul = self.W[i] @ np.transpose(self.A[i])\n",
    "```\n",
    "- **Explanation**: Calculate the weighted sum of inputs from the previous layer. This is the dot product of the weights (`self.W[i]`) and the activations from the previous layer (`self.A[i]`).\n",
    "- **Mathematical Notation**: $ Z^{(l)} = W^{(l)} \\cdot A^{(l-1)} $\n",
    "- **Chain Rule Application**: Not directly applicable here, but this prepares for the activation function.\n",
    "\n",
    "#### 3. Adjust Shape for Computation\n",
    "\n",
    "```python\n",
    "shape = Mat_Vec_Mul.shape\n",
    "Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[i]\n",
    "```\n",
    "- **Explanation**: Adjust the shape of the matrix-vector multiplication result and add the bias term.\n",
    "- **Mathematical Notation**: $ Z^{(l)} = W^{(l)} \\cdot A^{(l-1)} + b^{(l)} $\n",
    "\n",
    "#### 4. Activation Function\n",
    "\n",
    "```python\n",
    "self.Z.append(Z)\n",
    "A = self.activation_hidden[0](Z)\n",
    "self.A.append(A)\n",
    "```\n",
    "- **Explanation**: Apply the activation function to the weighted sum `Z` to get the activation for the current layer. Store `Z` and `A` for later use.\n",
    "- **Mathematical Notation**: $ A^{(l)} = \\sigma(Z^{(l)}) $\n",
    "- **Chain Rule Application**: This prepares the activations for the next layer and for backpropagation.\n",
    "\n",
    "#### 5. Output Layer Weighted Sum\n",
    "\n",
    "```python\n",
    "Mat_Vec_Mul = self.W[-1] @ np.transpose(self.A[-1])\n",
    "shape = Mat_Vec_Mul.shape\n",
    "Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[-1]\n",
    "```\n",
    "- **Explanation**: Calculate the weighted sum for the output layer.\n",
    "- **Mathematical Notation**: $ Z^{(L)} = W^{(L)} \\cdot A^{(L-1)} + b^{(L)} $\n",
    "\n",
    "#### 6. Output Layer Activation Function\n",
    "\n",
    "```python\n",
    "self.Z.append(Z)\n",
    "A = self.activation_output[0](Z)\n",
    "self.A.append(A)\n",
    "```\n",
    "- **Explanation**: Apply the activation function to the weighted sum `Z` of the output layer to get the final output.\n",
    "- **Mathematical Notation**: $ \\hat{y} = \\sigma(Z^{(L)}) $\n",
    "\n",
    "#### 7. Compute Loss\n",
    "\n",
    "```python\n",
    "Loss = self.Loss()\n",
    "return Loss\n",
    "```\n",
    "- **Explanation**: Calculate the loss function to measure the difference between the predicted values and the actual values.\n",
    "- **Mathematical Notation**: $ L = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y} - y)^2 $\n",
    "\n",
    "### Summary\n",
    "\n",
    "The forward pass involves computing the weighted sum and applying the activation function for each layer sequentially from the input to the output layer. Each line of code in the forward pass implements these steps, setting up the network for backpropagation by storing the activations and weighted sums needed for calculating gradients. The chain rule is implicitly applied in the sense that each layer's output becomes the input for the next layer, ensuring that the gradients can be propagated backward during the training process.\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Sure! Let's delve into the backward pass of the neural network, explaining each line and the underlying mathematics, including the application of the chain rule of derivatives.\n",
    "\n",
    "### Backward Pass Explained\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:679/0*9lo2ux8ASvt6YJkH.gif\"/>\n",
    "\n",
    "\n",
    "```python\n",
    "def Backwardpass(self):\n",
    "    # Output layer error\n",
    "    output_error = (self.A[-1] - self.idealValues)\n",
    "    output_delta = output_error * self.activation_output_derivative(self.Z[-1])  ## delta(L) = Error * d(sigma(L))/dz = activation_output_derivative(Z(L))\n",
    "\n",
    "    # Reshape for correct dimensions\n",
    "    output_delta = output_delta.reshape(-1, 1)\n",
    "    self.A[-2] = self.A[-2].reshape(1, -1)\n",
    "\n",
    "    # Gradients for output layer\n",
    "    dW = output_delta @ self.A[-2]\n",
    "    db = np.sum(output_delta, axis=1)\n",
    "\n",
    "    self.W[-1] -= self.learning_rate * dW\n",
    "    self.b[-1] -= self.learning_rate * db\n",
    "\n",
    "    # Backpropagate through hidden layers\n",
    "    delta = output_delta\n",
    "    for i in range(len(self.W) - 2, -1, -1):\n",
    "        delta = (self.W[i + 1].T @ delta).reshape(-1) * self.activation_hidden_derivative(self.Z[i])  ## W(L+1,L)^T * delta(L) * d(sigma(L-i))/dz = activation_hidden_derivative(self.Z[i])\n",
    "        \n",
    "        # Reshape for correct dimensions\n",
    "        delta = delta.reshape(-1, 1)\n",
    "        self.A[i] = self.A[i].reshape(1, -1)\n",
    "        \n",
    "        dW = delta @ self.A[i]\n",
    "        db = np.sum(delta, axis=1)\n",
    "        \n",
    "        self.W[i] -= self.learning_rate * dW\n",
    "        self.b[i] -= self.learning_rate * db\n",
    "```\n",
    "\n",
    "### Mathematical Explanation and Chain Rule Application\n",
    "\n",
    "#### 1. Output Layer Error\n",
    "\n",
    "```python\n",
    "output_error = (self.A[-1] - self.idealValues)\n",
    "```\n",
    "- **Explanation**: The output layer error is the difference between the predicted values (`self.A[-1]`) and the ideal (target) values (`self.idealValues`).\n",
    "- **Mathematical Notation**: $ E = \\hat{y} - y $\n",
    "\n",
    "#### 2. Output Delta\n",
    "\n",
    "```python\n",
    "output_delta = output_error * self.activation_output_derivative(self.Z[-1])\n",
    "```\n",
    "- **Explanation**: Multiply the error by the derivative of the activation function of the output layer. This gives the gradient of the loss with respect to the weighted sum $ Z $ of the output layer.\n",
    "- **Mathematical Notation**: $ \\delta^{(L)} = E \\cdot \\sigma' (Z^{(L)}) $\n",
    "- **Chain Rule Application**:\n",
    "  - Error: $ \\frac{\\partial L}{\\partial \\hat{y}} $\n",
    "  - Activation derivative: $ \\frac{\\partial \\hat{y}}{\\partial Z^{(L)}} $\n",
    "  - Combined: $ \\delta^{(L)} = \\frac{\\partial L}{\\partial Z^{(L)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial Z^{(L)}} $\n",
    "\n",
    "#### 3. Reshape for Correct Dimensions\n",
    "\n",
    "```python\n",
    "output_delta = output_delta.reshape(-1, 1)\n",
    "self.A[-2] = self.A[-2].reshape(1, -1)\n",
    "```\n",
    "- **Explanation**: Reshape the delta and the activations for matrix multiplication.\n",
    "\n",
    "#### 4. Gradients for Output Layer\n",
    "\n",
    "```python\n",
    "dW = output_delta @ self.A[-2]\n",
    "db = np.sum(output_delta, axis=1)\n",
    "```\n",
    "- **Explanation**: Calculate the gradients for the weights and biases in the output layer.\n",
    "- **Mathematical Notation**:\n",
    "  - Weight gradients: $ \\frac{\\partial L}{\\partial W^{(L)}} = \\delta^{(L)} \\cdot A^{(L-1)} $\n",
    "  - Bias gradients: $ \\frac{\\partial L}{\\partial b^{(L)}} = \\delta^{(L)} $\n",
    "- **Chain Rule Application**:\n",
    "  - For weights: $ \\frac{\\partial L}{\\partial W^{(L)}} = \\frac{\\partial L}{\\partial Z^{(L)}} \\cdot \\frac{\\partial Z^{(L)}}{\\partial W^{(L)}} $\n",
    "  - For biases: $ \\frac{\\partial L}{\\partial b^{(L)}} = \\frac{\\partial L}{\\partial Z^{(L)}} \\cdot \\frac{\\partial Z^{(L)}}{\\partial b^{(L)}} $\n",
    "\n",
    "#### 5. Update Output Layer Weights and Biases\n",
    "\n",
    "```python\n",
    "self.W[-1] -= self.learning_rate * dW\n",
    "self.b[-1] -= self.learning_rate * db\n",
    "```\n",
    "- **Explanation**: Update the weights and biases using the calculated gradients and the learning rate.\n",
    "- **Mathematical Notation**:\n",
    "  - Weights update: $ W^{(L)} \\leftarrow W^{(L)} - \\eta \\cdot \\frac{\\partial L}{\\partial W^{(L)}} $\n",
    "  - Biases update: $ b^{(L)} \\leftarrow b^{(L)} - \\eta \\cdot \\frac{\\partial L}{\\partial b^{(L)}} $\n",
    "\n",
    "#### 6. Backpropagate Through Hidden Layers\n",
    "\n",
    "```python\n",
    "delta = output_delta\n",
    "for i in range(len(self.W) - 2, -1, -1):\n",
    "    delta = (self.W[i + 1].T @ delta).reshape(-1) * self.activation_hidden_derivative(self.Z[i])\n",
    "```\n",
    "- **Explanation**: Backpropagate the delta through each hidden layer.\n",
    "- **Mathematical Notation**:\n",
    "  - For layer $ l $: $ \\delta^{(l)} = (\\delta^{(l+1)} \\cdot W^{(l+1)}) \\cdot \\sigma' (Z^{(l)}) $\n",
    "- **Chain Rule Application**:\n",
    "  - For hidden layers: $ \\delta^{(l)} = \\frac{\\partial L}{\\partial Z^{(l)}} = \\left( \\frac{\\partial L}{\\partial A^{(l+1)}} \\cdot \\frac{\\partial A^{(l+1)}}{\\partial Z^{(l+1)}} \\cdot \\frac{\\partial Z^{(l+1)}}{\\partial A^{(l)}} \\right) \\cdot \\frac{\\partial A^{(l)}}{\\partial Z^{(l)}} $\n",
    "\n",
    "#### 7. Reshape for Correct Dimensions\n",
    "\n",
    "```python\n",
    "delta = delta.reshape(-1, 1)\n",
    "self.A[i] = self.A[i].reshape(1, -1)\n",
    "```\n",
    "- **Explanation**: Reshape the delta and the activations for matrix multiplication.\n",
    "\n",
    "#### 8. Gradients for Hidden Layers\n",
    "\n",
    "```python\n",
    "dW = delta @ self.A[i]\n",
    "db = np.sum(delta, axis=1)\n",
    "```\n",
    "- **Explanation**: Calculate the gradients for the weights and biases in the hidden layers.\n",
    "- **Mathematical Notation**:\n",
    "  - Weight gradients: $ \\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot A^{(l-1)} $\n",
    "  - Bias gradients: $ \\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)} $\n",
    "- **Chain Rule Application**:\n",
    "  - For weights: $ \\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial Z^{(l)}} \\cdot \\frac{\\partial Z^{(l)}}{\\partial W^{(l)}} $\n",
    "  - For biases: $ \\frac{\\partial L}{\\partial b^{(l)}} = \\frac{\\partial L}{\\partial Z^{(l)}} \\cdot \\frac{\\partial Z^{(l)}}{\\partial b^{(l)}} $\n",
    "\n",
    "#### 9. Update Hidden Layer Weights and Biases\n",
    "\n",
    "```python\n",
    "self.W[i] -= self.learning_rate * dW\n",
    "self.b[i] -= self.learning_rate * db\n",
    "```\n",
    "- **Explanation**: Update the weights and biases using the calculated gradients and the learning rate.\n",
    "- **Mathematical Notation**:\n",
    "  - Weights update: $ W^{(l)} \\leftarrow W^{(l)} - \\eta \\cdot \\frac{\\partial L}{\\partial W^{(l)}} $\n",
    "  - Biases update: $ b^{(l)} \\leftarrow b^{(l)} - \\eta \\cdot \\frac{\\partial L}{\\partial b^{(l)}} $\n",
    "\n",
    "### Summary\n",
    "\n",
    "The backward pass involves computing the error and gradients for each layer starting from the output layer and moving backward through the hidden layers. The chain rule is applied to propagate the error gradients backward, allowing the network to update its weights and biases to minimize the loss function. Each line of code in the backward pass implements a step in this gradient descent optimization process.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
