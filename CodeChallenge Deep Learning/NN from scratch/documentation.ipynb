{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's a concise explanation for your documentation:\n",
    "\n",
    "---\n",
    "\n",
    "### NeuralNetwork Class\n",
    "\n",
    "The `NeuralNetwork` class implements a basic feedforward neural network capable of training and making predictions. It accepts the following parameters during initialization:\n",
    "\n",
    "- **`input`**: Represents the input data or features for the neural network.\n",
    "- **`hiddenlayer`**: Specifies the number of neurons in each hidden layer as a list of integers.\n",
    "- **`activation_hidden`**: Tuple containing the activation function and its derivative for the hidden layers.\n",
    "- **`outputlayer`**: Specifies the number of neurons in the output layer.\n",
    "- **`idealValues`**: Target output values for training purposes.\n",
    "- **`activation_output`**: Tuple containing the activation function and its derivative for the output layer.\n",
    "- **`learning_rate`**: Determines the step size in gradient descent optimization during training.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "# Example instantiation\n",
    "NN = NeuralNetwork(\n",
    "    input=[1, 2, 3],\n",
    "    hiddenlayer=[4],\n",
    "    activation_hidden=(activations.LeakyRelu, activations.LeakyRelu_derivative),\n",
    "    outputlayer=2,\n",
    "    idealValues=[1, 2],\n",
    "    activation_output=(activations.sigmoid, activations.sigmoid_derivative),\n",
    "    learning_rate=0.001\n",
    ")\n",
    "```\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The `NeuralNetwork` class encapsulates the functionality for creating and training a neural network model. It provides methods to initialize the network, perform forward and backward propagation, compute loss, and update model parameters using gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/blt790f1b7ac4e04301/6543ff50fcf447040a6b8dc7/News_Image_(47).png?width=1280&auto=webp&quality=95&format=jpg&disable=upscale\"\n",
    "width=500\n",
    "heigth=500\n",
    "/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Sure! Let's go through the forward pass code, explaining each line and the underlying mathematical concepts.\n",
    "\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D4D22AQH-8ATyCCV_Ww/feedshare-shrink_800/0/1720882385473?e=1724284800&v=beta&t=wc2S7yF2Fo5zIDEjMpC-VdjkvJEg9K4m3mTTHPnknDU\"/>\n",
    "\n",
    "### Forward Pass Explained\n",
    "\n",
    "```python\n",
    "def Forwardpass(self):\n",
    "    for i in range(len(self.W) - 1):\n",
    "        Mat_Vec_Mul = self.W[i] @ np.transpose(self.A[i])  # weight and activation(L-1) multiplication , weight matrix of (L) and (L-1)\n",
    "        \n",
    "        shape = Mat_Vec_Mul.shape  # to adjust the shape from (i x 1 ) of vectors to (i,) to reduce errors in computation\n",
    "        Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[i]  # Z = wx + b --> weighted sum\n",
    "        self.Z.append(Z)\n",
    "        A = self.activation_hidden[0](Z)\n",
    "        self.A.append(A)\n",
    "        \n",
    "    Mat_Vec_Mul = self.W[-1] @ np.transpose(self.A[-1]) \n",
    "    shape = Mat_Vec_Mul.shape  # to adjust the shape from (i x 1 ) --> (i,) to reduce errors in computation\n",
    "    Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[-1]\n",
    "    self.Z.append(Z)\n",
    "    A = self.activation_output[0](Z)\n",
    "    self.A.append(A)\n",
    "    Loss = self.Loss()\n",
    "    return Loss\n",
    "```\n",
    "\n",
    "### Mathematical Explanation and Chain Rule Application\n",
    "\n",
    "#### 1. Initialize the Forward Pass\n",
    "\n",
    "```python\n",
    "for i in range(len(self.W) - 1):\n",
    "```\n",
    "- **Explanation**: Iterate through each layer except the last one (output layer).\n",
    "\n",
    "#### 2. Weighted Sum Calculation\n",
    "\n",
    "```python\n",
    "Mat_Vec_Mul = self.W[i] @ np.transpose(self.A[i])\n",
    "```\n",
    "- **Explanation**: Calculate the weighted sum of inputs from the previous layer. This is the dot product of the weights (`self.W[i]`) and the activations from the previous layer (`self.A[i]`).\n",
    "- **Mathematical Notation**: $ Z^{(l)} = W^{(l)} \\cdot A^{(l-1)} $\n",
    "- **Chain Rule Application**: Not directly applicable here, but this prepares for the activation function.\n",
    "\n",
    "#### 3. Adjust Shape for Computation\n",
    "\n",
    "```python\n",
    "shape = Mat_Vec_Mul.shape\n",
    "Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[i]\n",
    "```\n",
    "- **Explanation**: Adjust the shape of the matrix-vector multiplication result and add the bias term.\n",
    "- **Mathematical Notation**: $ Z^{(l)} = W^{(l)} \\cdot A^{(l-1)} + b^{(l)} $\n",
    "\n",
    "#### 4. Activation Function\n",
    "\n",
    "```python\n",
    "self.Z.append(Z)\n",
    "A = self.activation_hidden[0](Z)\n",
    "self.A.append(A)\n",
    "```\n",
    "- **Explanation**: Apply the activation function to the weighted sum `Z` to get the activation for the current layer. Store `Z` and `A` for later use.\n",
    "- **Mathematical Notation**: $ A^{(l)} = \\sigma(Z^{(l)}) $\n",
    "- **Chain Rule Application**: This prepares the activations for the next layer and for backpropagation.\n",
    "\n",
    "#### 5. Output Layer Weighted Sum\n",
    "\n",
    "```python\n",
    "Mat_Vec_Mul = self.W[-1] @ np.transpose(self.A[-1])\n",
    "shape = Mat_Vec_Mul.shape\n",
    "Z = (Mat_Vec_Mul.reshape(shape[0])) + self.b[-1]\n",
    "```\n",
    "- **Explanation**: Calculate the weighted sum for the output layer.\n",
    "- **Mathematical Notation**: $ Z^{(L)} = W^{(L)} \\cdot A^{(L-1)} + b^{(L)} $\n",
    "\n",
    "#### 6. Output Layer Activation Function\n",
    "\n",
    "```python\n",
    "self.Z.append(Z)\n",
    "A = self.activation_output[0](Z)\n",
    "self.A.append(A)\n",
    "```\n",
    "- **Explanation**: Apply the activation function to the weighted sum `Z` of the output layer to get the final output.\n",
    "- **Mathematical Notation**: $ \\hat{y} = \\sigma(Z^{(L)}) $\n",
    "\n",
    "#### 7. Compute Loss\n",
    "\n",
    "```python\n",
    "Loss = self.Loss()\n",
    "return Loss\n",
    "```\n",
    "- **Explanation**: Calculate the loss function to measure the difference between the predicted values and the actual values.\n",
    "- **Mathematical Notation**: $ L = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y} - y)^2 $\n",
    "\n",
    "### Summary\n",
    "\n",
    "The forward pass involves computing the weighted sum and applying the activation function for each layer sequentially from the input to the output layer. Each line of code in the forward pass implements these steps, setting up the network for backpropagation by storing the activations and weighted sums needed for calculating gradients. The chain rule is implicitly applied in the sense that each layer's output becomes the input for the next layer, ensuring that the gradients can be propagated backward during the training process.\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Sure! Let's delve into the backward pass of the neural network, explaining each line and the underlying mathematics, including the application of the chain rule of derivatives.\n",
    "\n",
    "### Backward Pass Explained\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:679/0*9lo2ux8ASvt6YJkH.gif\"/>\n",
    "\n",
    "\n",
    "```python\n",
    "def Backwardpass(self):\n",
    "    # Output layer error\n",
    "    output_error = (self.A[-1] - self.idealValues)\n",
    "    output_delta = output_error * self.activation_output_derivative(self.Z[-1])  ## delta(L) = Error * d(sigma(L))/dz = activation_output_derivative(Z(L))\n",
    "\n",
    "    # Reshape for correct dimensions\n",
    "    output_delta = output_delta.reshape(-1, 1)\n",
    "    self.A[-2] = self.A[-2].reshape(1, -1)\n",
    "\n",
    "    # Gradients for output layer\n",
    "    dW = output_delta @ self.A[-2]\n",
    "    db = np.sum(output_delta, axis=1)\n",
    "\n",
    "    self.W[-1] -= self.learning_rate * dW\n",
    "    self.b[-1] -= self.learning_rate * db\n",
    "\n",
    "    # Backpropagate through hidden layers\n",
    "    delta = output_delta\n",
    "    for i in range(len(self.W) - 2, -1, -1):\n",
    "        delta = (self.W[i + 1].T @ delta).reshape(-1) * self.activation_hidden_derivative(self.Z[i])  ## W(L+1,L)^T * delta(L) * d(sigma(L-i))/dz = activation_hidden_derivative(self.Z[i])\n",
    "        \n",
    "        # Reshape for correct dimensions\n",
    "        delta = delta.reshape(-1, 1)\n",
    "        self.A[i] = self.A[i].reshape(1, -1)\n",
    "        \n",
    "        dW = delta @ self.A[i]\n",
    "        db = delta.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.W[i] -= self.learning_rate * dW\n",
    "        self.b[i] -= self.learning_rate * db\n",
    "```\n",
    "\n",
    "### Mathematical Explanation and Chain Rule Application\n",
    "\n",
    "#### 1. Output Layer Error\n",
    "\n",
    "```python\n",
    "output_error = (self.A[-1] - self.idealValues)\n",
    "```\n",
    "- **Explanation**: The output layer error is the difference between the predicted values (`self.A[-1]`) and the ideal (target) values (`self.idealValues`).\n",
    "- **Mathematical Notation**: $ E = \\hat{y} - y $\n",
    "\n",
    "#### 2. Output Delta\n",
    "\n",
    "```python\n",
    "output_delta = output_error * self.activation_output_derivative(self.Z[-1])\n",
    "```\n",
    "- **Explanation**: Multiply the error by the derivative of the activation function of the output layer. This gives the gradient of the loss with respect to the weighted sum $ Z $ of the output layer.\n",
    "- **Mathematical Notation**: $ \\delta^{(L)} = E \\cdot \\sigma' (Z^{(L)}) $\n",
    "- **Chain Rule Application**:\n",
    "  - Error: $ \\frac{\\partial L}{\\partial \\hat{y}} $\n",
    "  - Activation derivative: $ \\frac{\\partial \\hat{y}}{\\partial Z^{(L)}} $\n",
    "  - Combined: $ \\delta^{(L)} = \\frac{\\partial L}{\\partial Z^{(L)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial Z^{(L)}} $\n",
    "\n",
    "#### 3. Reshape for Correct Dimensions\n",
    "\n",
    "```python\n",
    "output_delta = output_delta.reshape(-1, 1)\n",
    "self.A[-2] = self.A[-2].reshape(1, -1)\n",
    "```\n",
    "- **Explanation**: Reshape the delta and the activations for matrix multiplication.\n",
    "\n",
    "#### 4. Gradients for Output Layer\n",
    "\n",
    "```python\n",
    "dW = output_delta @ self.A[-2]\n",
    "db = np.sum(output_delta, axis=1)\n",
    "```\n",
    "- **Explanation**: Calculate the gradients for the weights and biases in the output layer.\n",
    "- **Mathematical Notation**:\n",
    "  - Weight gradients: $ \\frac{\\partial L}{\\partial W^{(L)}} = \\delta^{(L)} \\cdot A^{(L-1)} $\n",
    "  - Bias gradients: $ \\frac{\\partial L}{\\partial b^{(L)}} = \\delta^{(L)} $\n",
    "- **Chain Rule Application**:\n",
    "  - For weights: $ \\frac{\\partial L}{\\partial W^{(L)}} = \\frac{\\partial L}{\\partial Z^{(L)}} \\cdot \\frac{\\partial Z^{(L)}}{\\partial W^{(L)}} $\n",
    "  - For biases: $ \\frac{\\partial L}{\\partial b^{(L)}} = \\frac{\\partial L}{\\partial Z^{(L)}} \\cdot \\frac{\\partial Z^{(L)}}{\\partial b^{(L)}} $\n",
    "\n",
    "#### 5. Update Output Layer Weights and Biases\n",
    "\n",
    "```python\n",
    "self.W[-1] -= self.learning_rate * dW\n",
    "self.b[-1] -= self.learning_rate * db\n",
    "```\n",
    "- **Explanation**: Update the weights and biases using the calculated gradients and the learning rate.\n",
    "- **Mathematical Notation**:\n",
    "  - Weights update: $ W^{(L)} \\leftarrow W^{(L)} - \\eta \\cdot \\frac{\\partial L}{\\partial W^{(L)}} $\n",
    "  - Biases update: $ b^{(L)} \\leftarrow b^{(L)} - \\eta \\cdot \\frac{\\partial L}{\\partial b^{(L)}} $\n",
    "\n",
    "#### 6. Backpropagate Through Hidden Layers\n",
    "\n",
    "```python\n",
    "delta = output_delta\n",
    "for i in range(len(self.W) - 2, -1, -1):\n",
    "    delta = (self.W[i + 1].T @ delta).reshape(-1) * self.activation_hidden_derivative(self.Z[i])\n",
    "```\n",
    "- **Explanation**: Backpropagate the delta through each hidden layer.\n",
    "- **Mathematical Notation**:\n",
    "  - For layer $ l $: $ \\delta^{(l)} = (\\delta^{(l+1)} \\cdot W^{(l+1)}) \\cdot \\sigma' (Z^{(l)}) $\n",
    "- **Chain Rule Application**:\n",
    "  - For hidden layers: $ \\delta^{(l)} = \\frac{\\partial L}{\\partial Z^{(l)}} = \\left( \\frac{\\partial L}{\\partial A^{(l+1)}} \\cdot \\frac{\\partial A^{(l+1)}}{\\partial Z^{(l+1)}} \\cdot \\frac{\\partial Z^{(l+1)}}{\\partial A^{(l)}} \\right) \\cdot \\frac{\\partial A^{(l)}}{\\partial Z^{(l)}} $\n",
    "\n",
    "#### 7. Reshape for Correct Dimensions\n",
    "\n",
    "```python\n",
    "delta = delta.reshape(-1, 1)\n",
    "self.A[i] = self.A[i].reshape(1, -1)\n",
    "```\n",
    "- **Explanation**: Reshape the delta and the activations for matrix multiplication.\n",
    "\n",
    "#### 8. Gradients for Hidden Layers\n",
    "\n",
    "```python\n",
    "dW = delta @ self.A[i]\n",
    "db = np.sum(delta, axis=1)\n",
    "```\n",
    "- **Explanation**: Calculate the gradients for the weights and biases in the hidden layers.\n",
    "- **Mathematical Notation**:\n",
    "  - Weight gradients: $ \\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot A^{(l-1)} $\n",
    "  - Bias gradients: $ \\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)} $\n",
    "- **Chain Rule Application**:\n",
    "  - For weights: $ \\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial Z^{(l)}} \\cdot \\frac{\\partial Z^{(l)}}{\\partial W^{(l)}} $\n",
    "  - For biases: $ \\frac{\\partial L}{\\partial b^{(l)}} = \\frac{\\partial L}{\\partial Z^{(l)}} \\cdot \\frac{\\partial Z^{(l)}}{\\partial b^{(l)}} $\n",
    "\n",
    "#### 9. Update Hidden Layer Weights and Biases\n",
    "\n",
    "```python\n",
    "self.W[i] -= self.learning_rate * dW\n",
    "self.b[i] -= self.learning_rate * db\n",
    "```\n",
    "- **Explanation**: Update the weights and biases using the calculated gradients and the learning rate.\n",
    "- **Mathematical Notation**:\n",
    "  - Weights update: $ W^{(l)} \\leftarrow W^{(l)} - \\eta \\cdot \\frac{\\partial L}{\\partial W^{(l)}} $\n",
    "  - Biases update: $ b^{(l)} \\leftarrow b^{(l)} - \\eta \\cdot \\frac{\\partial L}{\\partial b^{(l)}} $\n",
    "\n",
    "### Summary\n",
    "\n",
    "The backward pass involves computing the error and gradients for each layer starting from the output layer and moving backward through the hidden layers. The chain rule is applied to propagate the error gradients backward, allowing the network to update its weights and biases to minimize the loss function. Each line of code in the backward pass implements a step in this gradient descent optimization process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Batch Gradient Descent \n",
    "\n",
    "<h3> Batch gradient descent updates the weights on the entire training set in a single epoch </h3>\n",
    "\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D4D22AQH-8ATyCCV_Ww/feedshare-shrink_800/0/1720882385473?e=1724284800&v=beta&t=wc2S7yF2Fo5zIDEjMpC-VdjkvJEg9K4m3mTTHPnknDU\"/>\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "To explain how to train a neural network using batch gradient descent with the weights and biases updated based on the entire dataset, let's work through a simple architecture and demonstrate the process with symbolic matrices and vectors. \n",
    "\n",
    "### Architecture and Shapes\n",
    "\n",
    "Letâ€™s assume the architecture as follows:\n",
    "- **Input Layer:** 3 nodes\n",
    "- **Hidden Layer:** 2 nodes\n",
    "- **Output Layer:** 3 nodes\n",
    "\n",
    "Here's how the matrices and vectors are shaped:\n",
    "\n",
    "1. **Input Matrix ($\\mathbf{X}$)**: Shape $(m, 3)$\n",
    "   - $m$ is the number of training examples.\n",
    "\n",
    "2. **Hidden Layer Weight Matrix ($\\mathbf{W}_1$)**: Shape $(3, 2)$\n",
    "   - Each column corresponds to the weights connecting one input node to each hidden node.\n",
    "\n",
    "3. **Hidden Layer Bias Vector ($\\mathbf{b}_1$)**: Shape $(1, 2)$\n",
    "   - One bias per hidden node.\n",
    "\n",
    "4. **Output Layer Weight Matrix ($\\mathbf{W}_2$)**: Shape $(2, 3)$\n",
    "   - Each column corresponds to the weights connecting one hidden node to each output node.\n",
    "\n",
    "5. **Output Layer Bias Vector ($\\mathbf{b}_2$)**: Shape $(1, 3)$\n",
    "   - One bias per output node.\n",
    "\n",
    "### Example with Symbolic Elements\n",
    "\n",
    "Let's work with symbolic elements to clarify the process:\n",
    "\n",
    "#### **1. Forward Pass**\n",
    "\n",
    "**Input Matrix ($\\mathbf{X}$)**: Suppose we have 2 training examples.\n",
    "$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "Here, $\\mathbf{X}$ has shape $(2, 3)$ where each row represents a training example.\n",
    "\n",
    "**Hidden Layer Weight Matrix ($\\mathbf{W}_1$)**:\n",
    "$\n",
    "\\mathbf{W}_1 = \\begin{bmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "Here, $\\mathbf{W}_1$ has shape $(3, 2)$, connecting each of the 3 input nodes to each of the 2 hidden nodes.\n",
    "\n",
    "**Hidden Layer Bias Vector ($\\mathbf{b}_1$)**:\n",
    "$\n",
    "\\mathbf{b}_1 = \\begin{bmatrix}\n",
    "b_{1} & b_{2}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "Here, $\\mathbf{b}_1$ has shape $(1, 2)$, with each element corresponding to a bias term for a hidden node.\n",
    "\n",
    "**Compute Hidden Layer Activations ($\\mathbf{A}_1$)**:\n",
    "$\n",
    "\\mathbf{Z}_1 = \\mathbf{X} \\cdot \\mathbf{W}_1 + \\mathbf{b}_1\n",
    "$\n",
    "$\n",
    "\\mathbf{Z}_1 = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23}\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "b_{1} & b_{2}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathbf{A}_1 = \\sigma(\\mathbf{Z}_1)\n",
    "$\n",
    "where $\\sigma$ is the activation function (e.g., sigmoid, tanh).\n",
    "\n",
    "**Output Layer Weight Matrix ($\\mathbf{W}_2$)**:\n",
    "$\n",
    "\\mathbf{W}_2 = \\begin{bmatrix}\n",
    "w_{1} & w_{2} & w_{3} \\\\\n",
    "w_{4} & w_{5} & w_{6}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "Here, $\\mathbf{W}_2$ has shape $(2, 3)$, connecting each of the 2 hidden nodes to each of the 3 output nodes.\n",
    "\n",
    "**Output Layer Bias Vector ($\\mathbf{b}_2$)**:\n",
    "$\n",
    "\\mathbf{b}_2 = \\begin{bmatrix}\n",
    "b_{3} & b_{4} & b_{5}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "Here, $\\mathbf{b}_2$ has shape $(1, 3)$, with each element corresponding to a bias term for an output node.\n",
    "\n",
    "**Compute Output Layer Activations ($\\mathbf{A}_2$)**:\n",
    "$\n",
    "\\mathbf{Z}_2 = \\mathbf{A}_1 \\cdot \\mathbf{W}_2 + \\mathbf{b}_2\n",
    "$\n",
    "$\n",
    "\\mathbf{Z}_2 = \\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "w_{1} & w_{2} & w_{3} \\\\\n",
    "w_{4} & w_{5} & w_{6}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "b_{3} & b_{4} & b_{5}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathbf{A}_2 = \\text{softmax}(\\mathbf{Z}_2)\n",
    "$\n",
    "where softmax converts the logits into probabilities for classification tasks.\n",
    "\n",
    "#### **2. Compute Loss**\n",
    "\n",
    "Given the true labels ($\\mathbf{Y}$) and the predicted output ($\\mathbf{A}_2$):\n",
    "$\n",
    "\\text{Loss} = \\frac{1}{m} \\sum_{i=1}^{m} \\text{loss}(\\mathbf{Y}_i, \\mathbf{A}_{2i})\n",
    "$\n",
    "\n",
    "#### **3. Backward Pass**\n",
    "\n",
    "Calculate gradients for weights and biases.\n",
    "\n",
    "**Gradient for Output Layer:**\n",
    "$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{A}_2} = \\mathbf{A}_2 - \\mathbf{Y}\n",
    "$\n",
    "$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{W}_2} = \\frac{1}{m} \\mathbf{A}_1^T \\cdot (\\mathbf{A}_2 - \\mathbf{Y})\n",
    "$\n",
    "$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{b}_2} = \\frac{1}{m} \\sum_{i=1}^{m} (\\mathbf{A}_{2i} - \\mathbf{Y}_i)\n",
    "$\n",
    "\n",
    "**Gradient for Hidden Layer:**\n",
    "$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{A}_1} = (\\mathbf{A}_2 - \\mathbf{Y}) \\cdot \\mathbf{W}_2^T\n",
    "$\n",
    "$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{Z}_1} = \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{A}_1} \\cdot \\sigma'(\\mathbf{Z}_1)\n",
    "$\n",
    "$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{W}_1} = \\frac{1}{m} \\mathbf{X}^T \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{Z}_1}\n",
    "$\n",
    "$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\mathbf{b}_1} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{Z}_{1i}}\n",
    "$\n",
    "\n",
    "#### **4. Update Weights and Biases**\n",
    "\n",
    "Update weights and biases using the gradients:\n",
    "\n",
    "$\n",
    "\\mathbf{W}_2 = \\mathbf{W}_2 - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{W}_2}\n",
    "$\n",
    "$\n",
    "\\mathbf{b}_2 = \\mathbf{b}_2 - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{b}_2}\n",
    "$\n",
    "$\n",
    "\\mathbf{W}_1 = \\mathbf{W}_1 - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{W}_1}\n",
    "$\n",
    "$\n",
    "\\mathbf{b}_1 = \\mathbf{b}_1 - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{b}_1}\n",
    "$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary:\n",
    "- **Forward Pass**: Compute activations using the input data for the entire dataset.\n",
    "- **Compute Loss**: Evaluate the loss over the entire dataset.\n",
    "- **Backward Pass**: Calculate gradients using the loss function and update weights and biases.\n",
    "- **Update Parameters**: Adjust weights and biases using the gradients obtained.\n",
    "\n",
    "By processing the entire dataset in one epoch, the network updates its parameters based on the average gradients, which typically results in more stable and effective learning.\n",
    "\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
